import os
import cv2
import numpy as np
from skimage.feature import hog, local_binary_pattern
from torchvision.datasets import ImageFolder
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.decomposition import PCA

#Histogram of gradients (edges) and linear binary patterns (texture)
HOG_PARAMS = dict(orientations=9, pixels_per_cell=(8, 8),
                  cells_per_block=(2, 2), block_norm='L2-Hys')
LBP_RADIUS = 1
LBP_POINTS = 8 * LBP_RADIUS
LBP_METHOD = 'uniform'
PCA_DIM = 511  #511 features to match the amount generated by resnet18

#HOG and LBP only work on grayscale
def to_grayscale(img_tensor):
    img = transforms.ToPILImage()(img_tensor)
    gray = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)
    return gray

# Extract HOG + LBP features
def extract_hog_lbp_features(gray_img):
    hog_feat = hog(gray_img, **HOG_PARAMS)
    lbp = local_binary_pattern(gray_img, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, LBP_POINTS + 3), range=(0, LBP_POINTS + 2), density=True)
    return np.concatenate([hog_feat, lbp_hist])

# Extract from folder
def extract_features_from_folder(data_dir):
    transform = transforms.Compose([transforms.ToTensor()])
    dataset = ImageFolder(root=data_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

    features_list = []
    labels = []
    filenames = []
    class_names = dataset.classes

    for i, (img_tensor, label) in enumerate(dataloader):
        img_tensor = img_tensor.squeeze(0)
        gray_img = to_grayscale(img_tensor)
        features = extract_hog_lbp_features(gray_img)

        img_path, class_idx = dataset.samples[i]
        fname = os.path.basename(img_path)
        class_name = class_names[class_idx]

        filenames.append(fname)
        labels.append(class_name)
        features_list.append(features)

    return filenames, labels, np.array(features_list)

# Save features to .npz (compressed)
def save_reduced_features_to_npz(filenames, labels, X_reduced, filename_prefix):
    np.savez(
        f"{filename_prefix}.npz",
        features=X_reduced,
        filenames=np.array(filenames),
        labels=np.array(labels)
    )
    print(f"\n Saved compressed features to: {filename_prefix}.npz")

# Process test set with existing PCA
def process_test_set(test_path, pca_model, pca_dim):
    print("\nüîç Extracting features from test set...")
    test_filenames, test_labels, X_test = extract_features_from_folder(test_path)

    print(f"Initial test feature dimension: {X_test.shape[1]}")
    X_test_reduced = pca_model.transform(X_test)

    print(f" Test set reduced to {pca_dim} dimensions")
    save_reduced_features_to_npz(test_filenames, test_labels, X_test_reduced, "Test_hog_lbp_pca")

    print(f" Processed {len(test_filenames)} test images across {len(set(test_labels))} classes.")

# Main script
if __name__ == '__main__':
    data_path = 'data/train'

    print("üîç Extracting raw HOG + LBP features...")
    filenames, labels, X = extract_features_from_folder(data_path)

    print(f"\nInitial feature dimension: {X.shape[1]}")
    print(" Applying PCA reduction...")

    pca = PCA(n_components=PCA_DIM)
    X_reduced = pca.fit_transform(X)

    print(f" Reduced to {PCA_DIM} dimensions")
    print(f" Variance retained: {pca.explained_variance_ratio_.sum():.4f}")

    save_reduced_features_to_npz(filenames, labels, X_reduced, "Train_hog_lbp_pca")

    print(f"\n Processed {len(filenames)} training images across {len(set(labels))} classes.")

    # Apply PCA to test set
    process_test_set("data/test", pca, PCA_DIM)
